{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This file implements the neural network architecture for the synthetic data. First, the data is loaded in, labels are cleaned, and data is limited to only the posts/comments and the appropriate tag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_parquet(\"synthetic.parquet\")\n",
        "df[\"category\"] = df[\"label_type\"]\n",
        "df[\"text\"] = df[\"total_post\"]\n",
        "df = df[[\"text\", \"category\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we load the chosen tokenizer and define numerical labels for the categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "labels = {\n",
        "    \"republican\": 0,\n",
        "    \"democrat\": 1,\n",
        "    \"neutral\": 2,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is defined our dataset class, which is used for loading data into the neural net."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "\n",
        "        self.labels = [labels[label] for label in df['category']]\n",
        "        self.texts = [tokenizer(text, \n",
        "                               padding='max_length', max_length = 512, truncation=True,\n",
        "                                return_tensors=\"pt\") for text in df['text']]\n",
        "\n",
        "    def classes(self):\n",
        "        return self.labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        # Fetch a batch of labels\n",
        "        return np.array(self.labels[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        # Fetch a batch of inputs\n",
        "        return self.texts[idx]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "\n",
        "        return batch_texts, batch_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we split the data into train, validation, and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(112)\n",
        "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
        "                                     [int(.8*len(df)), int(.9*len(df))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we define the actual network and the training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout=0.5):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear1 = nn.Linear(768, 30)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(30,3)\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "\n",
        "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear1(dropout_output)\n",
        "        relu_layer = self.relu(linear_output)\n",
        "        linear_2 = self.linear2(relu_layer)\n",
        "        final_layer = self.softmax(linear_2)\n",
        "        return final_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, train_data, val_data, learning_rate, epochs):\n",
        "\n",
        "    train, val = Dataset(train_data), Dataset(val_data)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=4, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=4)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-7)\n",
        "\n",
        "    if use_cuda:\n",
        "        print(\"it's working!\")\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "        model.train()\n",
        "        total_acc_train = 0\n",
        "        total_loss_train = 0\n",
        "\n",
        "        for train_input, train_label in tqdm(train_dataloader):\n",
        "\n",
        "            train_label = train_label.to(device)\n",
        "            mask = train_input[\"attention_mask\"].to(device)\n",
        "            input_id = train_input[\"input_ids\"].squeeze(1).to(device)\n",
        "\n",
        "            output = model(input_id, mask)\n",
        "            batch_loss = criterion(\n",
        "                output, F.one_hot(train_label, num_classes=3).float()\n",
        "            )\n",
        "            total_loss_train += batch_loss.item() * 4\n",
        "\n",
        "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "            total_acc_train += acc\n",
        "\n",
        "            model.zero_grad()\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_acc_val = 0\n",
        "        total_loss_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for val_input, val_label in val_dataloader:\n",
        "\n",
        "                val_label = val_label.to(device)\n",
        "                mask = val_input[\"attention_mask\"].to(device)\n",
        "                input_id = val_input[\"input_ids\"].squeeze(1).to(device)\n",
        "\n",
        "                output = model(input_id, mask)\n",
        "\n",
        "                batch_loss = criterion(\n",
        "                    output, F.one_hot(val_label, num_classes=3).float()\n",
        "                )\n",
        "                total_loss_val += batch_loss.item() * 4\n",
        "\n",
        "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
        "                total_acc_val += acc\n",
        "\n",
        "        print(\n",
        "            f\"Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
        "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
        "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
        "                | Val Accuracy: {total_acc_val / len(val_data): .3f}\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After defining the model class, we defined remaining hyperparameters, and trained the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCeJvzQBE1Zd",
        "outputId": "bbe4c50f-4af8-4108-ae82-67998f963237"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 15\n",
        "LR = 1e-8\n",
        "model = BertClassifier()\n",
        "train(model, df_train, df_val, LR, EPOCHS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we define a function to evaluate our model on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "240TLEk9rfvS",
        "outputId": "1d2e11b5-7c61-4598-edbb-6a2d302955a4"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_data):\n",
        "    predictions = []\n",
        "\n",
        "    test = Dataset(test_data)\n",
        "\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=1)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "        model = model.cuda()\n",
        "\n",
        "    total_acc_test = 0\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for test_input, test_label in test_dataloader:\n",
        "            test_label = test_label.to(device)\n",
        "            mask = test_input[\"attention_mask\"].to(device)\n",
        "            input_id = test_input[\"input_ids\"].squeeze(1).to(device)\n",
        "            output = model(input_id, mask)\n",
        "            predictions.append(output.argmax(dim=1).item())\n",
        "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
        "            total_acc_test += acc\n",
        "\n",
        "    print(f\"Test Accuracy: {total_acc_test / len(test_data): .3f}\")\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we utilized that function to evaluate the test data, and then built a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output = evaluate(model, df_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1Wru3N3YlQu"
      },
      "outputs": [],
      "source": [
        "reverse = {\n",
        "    0: \"republican\",\n",
        "    1: \"democrat\",\n",
        "    2: \"neutral\",\n",
        "}\n",
        "df_test[\"predicted\"] = output\n",
        "df_test[\"actual\"] = df_test[\"category\"].map(labels)\n",
        "df_test[\"predicted\"] = df_test[\"predicted\"].map(reverse)\n",
        "df_test[\"actual\"] = df_test[\"actual\"].map(reverse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVF2lqMFX2Jb"
      },
      "outputs": [],
      "source": [
        "confusion_matrix = pd.crosstab(df_test['actual'], df_test['predicted'], rownames=['Actual'], colnames=['Predicted'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "YIicdpA4YETe",
        "outputId": "5e564399-fdcb-4d0a-fc7d-10b9e099c055"
      },
      "outputs": [],
      "source": [
        "confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "rnAkvLquYah5",
        "outputId": "8669abba-e604-4e8d-f5a6-d445028afe8f"
      },
      "outputs": [],
      "source": [
        "ax = sns.heatmap(confusion_matrix, annot=True, fmt=\"g\", cmap=\"Blues\")\n",
        "ax.set_title(\"Text Classificaiton Confusion Matrix\\n\\n\")\n",
        "ax.set_xlabel(\"\\nPredicted Values\")\n",
        "ax.set_ylabel(\"Actual Values \")\n",
        "ax.xaxis.set_ticklabels([\"Democrat\", \"Neutral\", \"Republican\"])\n",
        "ax.yaxis.set_ticklabels([\"Democrat\", \"Neutral\", \"Republican\"])\n",
        "plt.savefig(\"../30_outputs/synthetic_neural_net.png\", bbox_inches=\"tight\", dpi = 300)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we extract the precision and accuracy for each category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubz4oXvpZdI6",
        "outputId": "1bbb7741-9637-42f1-f2fa-52b2fcfc95e4"
      },
      "outputs": [],
      "source": [
        "print(metrics.classification_report(df_test[\"actual\"], df_test[\"predicted\"], target_names = [\"democrat\", \"neutral\", \"republican\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97IZGXgGYN5v"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"../20_models/model_synthetic\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "synthetic_net.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
